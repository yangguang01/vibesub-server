1
00:00:00,080 --> 00:00:03,760
We said, "Okay, we're going to go for

2
00:00:01,120 --> 00:00:05,200
AGI." 99% of the world thought we were

3
00:00:03,760 --> 00:00:06,879
crazy. 1% of the world they really

4
00:00:05,200 --> 00:00:08,480
resonated with. You know, in 10 or 20

5
00:00:06,879 --> 00:00:10,000
years, unless something goes hugely

6
00:00:08,480 --> 00:00:11,759
wrong, we'll have like unimaginable

7
00:00:10,000 --> 00:00:13,679
super intelligence. This is the best

8
00:00:11,759 --> 00:00:15,920
 time ever in the history of

9
00:00:13,679 --> 00:00:16,970
technology ever, period, to start a

10
00:00:15,920 --> 00:00:20,880
company.

11
00:00:16,970 --> 00:00:22,640
[Music]

12
00:00:20,880 --> 00:00:25,199
Well, Sam, thank you so much for joining

13
00:00:22,640 --> 00:00:28,480
us and thanks for all the inspiration. I

14
00:00:25,199 --> 00:00:30,960
mean, OpenAI itself uh is a true

15
00:00:28,480 --> 00:00:34,320
inspiration for any really really

16
00:00:30,960 --> 00:00:37,200
ambitious person. Um maybe we just start

17
00:00:34,320 --> 00:00:40,559
with that. I mean, what were some of the

18
00:00:37,200 --> 00:00:42,879
decisions early that seemed small that

19
00:00:40,559 --> 00:00:44,960
turned out to be incredibly pivotal? I

20
00:00:42,879 --> 00:00:47,680
mean, just deciding to do it was a big

21
00:00:44,960 --> 00:00:52,559
one. Like there we got very close to not

22
00:00:47,680 --> 00:00:55,120
starting OpenAI. Um AGI sounded crazy. I

23
00:00:52,559 --> 00:00:56,399
was I had Gary's job then and we were

24
00:00:55,120 --> 00:00:58,000
you know there was like all this other

25
00:00:56,399 --> 00:01:00,879
great stuff to do that would work all

26
00:00:58,000 --> 00:01:03,359
these great startups and AGI was like

27
00:01:00,879 --> 00:01:05,280
kind of a pipe dream and also even if it

28
00:01:03,359 --> 00:01:07,520
was possible deep mind seemed like

29
00:01:05,280 --> 00:01:09,200
impossibly far ahead and so we had this

30
00:01:07,520 --> 00:01:12,080
year over the course of 2015 where we

31
00:01:09,200 --> 00:01:14,640
were talking about starting it and you

32
00:01:12,080 --> 00:01:16,479
know it was like kind of coin flippy um

33
00:01:14,640 --> 00:01:18,320
and I think this is the the story of

34
00:01:16,479 --> 00:01:20,720
like many ambitious things where they

35
00:01:18,320 --> 00:01:22,799
seem so difficult and there's such good

36
00:01:20,720 --> 00:01:24,640
reasons not to do them that really takes

37
00:01:22,799 --> 00:01:25,680
a core of people that like sit in a

38
00:01:24,640 --> 00:01:28,159
room, look each other in the eye and

39
00:01:25,680 --> 00:01:29,600
say, "All right, let's do this." Uh, and

40
00:01:28,159 --> 00:01:31,119
those are like very important moments

41
00:01:29,600 --> 00:01:33,119
and I think when in doubt, you should

42
00:01:31,119 --> 00:01:36,479
lean into them. So, there were just a

43
00:01:33,119 --> 00:01:38,880
billion things, a billion reasons why

44
00:01:36,479 --> 00:01:41,280
people might say you shouldn't do it. I

45
00:01:38,880 --> 00:01:42,320
mean, off the bat, like even you one of

46
00:01:41,280 --> 00:01:44,400
the things you figured out was the

47
00:01:42,320 --> 00:01:46,399
scaling laws. It's so hard to remember

48
00:01:44,400 --> 00:01:49,200
what it was like. Uh, next year will be

49
00:01:46,399 --> 00:01:52,560
our 10 year anniversary and so not not

50
00:01:49,200 --> 00:01:52,560
Yeah. Thank you.

51
00:01:54,240 --> 00:01:59,759
But to to like remember what the vibes

52
00:01:56,960 --> 00:02:02,320
were like about AI 10 years ago, that

53
00:01:59,759 --> 00:02:04,159
was like way before the first language

54
00:02:02,320 --> 00:02:05,920
models that worked. We were trying to

55
00:02:04,159 --> 00:02:07,520
like play video games and we had this

56
00:02:05,920 --> 00:02:10,239
little robotic hand that could sort of

57
00:02:07,520 --> 00:02:13,360
barely do a Rubik's cube and we had no

58
00:02:10,239 --> 00:02:14,959
ideas for products, no revenue, no

59
00:02:13,360 --> 00:02:16,959
really idea that we were ever going to

60
00:02:14,959 --> 00:02:18,560
have revenue. And we were like sitting

61
00:02:16,959 --> 00:02:20,080
around at conference tables and

62
00:02:18,560 --> 00:02:22,800
whiteboards trying to come up with ideas

63
00:02:20,080 --> 00:02:25,120
for papers to write. It it was such it's

64
00:02:22,800 --> 00:02:27,520
it's like hard to explain now because it

65
00:02:25,120 --> 00:02:30,720
looks so obvious now how improbable it

66
00:02:27,520 --> 00:02:33,599
seemed at the time and how the idea of

67
00:02:30,720 --> 00:02:35,599
chatbt was like completely in the realm

68
00:02:33,599 --> 00:02:37,519
of science fiction. I mean, one of the

69
00:02:35,599 --> 00:02:40,959
things that really jumped out at me was

70
00:02:37,519 --> 00:02:43,599
uh you sort of, you know, rallied this

71
00:02:40,959 --> 00:02:46,319
idea that you should be working on AGI

72
00:02:43,599 --> 00:02:48,000
and then simultaneously you found the

73
00:02:46,319 --> 00:02:50,000
smartest people in the world who were

74
00:02:48,000 --> 00:02:52,800
working on that thing. That second part

75
00:02:50,000 --> 00:02:54,160
was sort of easier than it sounds. If if

76
00:02:52,800 --> 00:02:56,239
you say we're going to like do this

77
00:02:54,160 --> 00:02:58,480
crazy thing and it's and it's exciting

78
00:02:56,239 --> 00:03:00,400
and it's important if it works and other

79
00:02:58,480 --> 00:03:03,280
people aren't doing it, you can actually

80
00:03:00,400 --> 00:03:04,800
like get a lot of people together. Um,

81
00:03:03,280 --> 00:03:07,360
and so we were we said, "Okay, we're

82
00:03:04,800 --> 00:03:08,959
going to go for AGI." 99% of the world

83
00:03:07,360 --> 00:03:11,200
thought we were crazy. 1% of the world

84
00:03:08,959 --> 00:03:12,319
that really resonated with. Turned out

85
00:03:11,200 --> 00:03:14,560
there were a lot of smart people in that

86
00:03:12,319 --> 00:03:16,239
1% and you could get like there wasn't

87
00:03:14,560 --> 00:03:18,480
really anywhere else for them to go. So

88
00:03:16,239 --> 00:03:20,319
we were able to really concentrate the

89
00:03:18,480 --> 00:03:21,840
talent and it was a mission that people

90
00:03:20,319 --> 00:03:24,000
cared about. So even though it seemed

91
00:03:21,840 --> 00:03:27,120
unlikely, if it worked, it it seemed

92
00:03:24,000 --> 00:03:28,640
super valuable. And

93
00:03:27,120 --> 00:03:30,080
we've observed this many times with

94
00:03:28,640 --> 00:03:32,080
startups. If you are doing the same

95
00:03:30,080 --> 00:03:33,680
thing as everyone else, it is very hard

96
00:03:32,080 --> 00:03:35,200
to concentrate talent and it's very hard

97
00:03:33,680 --> 00:03:37,200
to get people to like really believe in

98
00:03:35,200 --> 00:03:39,680
a mission. And if you're doing like a

99
00:03:37,200 --> 00:03:41,440
oneofone thing,

100
00:03:39,680 --> 00:03:44,080
uh you you have a really nice tailwind

101
00:03:41,440 --> 00:03:45,599
there. Okay. So, some people in this

102
00:03:44,080 --> 00:03:48,159
room might be thinking like, should I

103
00:03:45,599 --> 00:03:51,120
try to start an OpenAI scale thing off

104
00:03:48,159 --> 00:03:53,760
the bat? Uh you know, you you also

105
00:03:51,120 --> 00:03:55,440
worked on Loop your first time around.

106
00:03:53,760 --> 00:03:57,200
You know, were there lessons from that?

107
00:03:55,440 --> 00:04:00,239
OpenAI was not an open eye scale thing

108
00:03:57,200 --> 00:04:01,920
off the bat. OpenAI was like eight

109
00:04:00,239 --> 00:04:03,760
people in a room and then it was 20

110
00:04:01,920 --> 00:04:05,760
people in a room and it was very unclear

111
00:04:03,760 --> 00:04:09,040
what to do and we were just like trying

112
00:04:05,760 --> 00:04:11,360
to write a good research paper. Um

113
00:04:09,040 --> 00:04:13,920
so the the things that eventually become

114
00:04:11,360 --> 00:04:16,079
really big do not start off that way. I

115
00:04:13,920 --> 00:04:18,479
think it's important to like dream that

116
00:04:16,079 --> 00:04:21,440
it could be big if it works. Nothing big

117
00:04:18,479 --> 00:04:23,360
starts that way. and and and

118
00:04:21,440 --> 00:04:25,440
Venode Kla has this quote that I've

119
00:04:23,360 --> 00:04:27,680
always liked which is there's a very big

120
00:04:25,440 --> 00:04:29,199
difference between a Z million dollar

121
00:04:27,680 --> 00:04:30,720
startup and a zero billion dollar

122
00:04:29,199 --> 00:04:32,000
startup

123
00:04:30,720 --> 00:04:33,520
but they both have zero dollars of

124
00:04:32,000 --> 00:04:35,280
revenue. They're both like a few people

125
00:04:33,520 --> 00:04:36,639
sitting in a room and you're both trying

126
00:04:35,280 --> 00:04:41,199
to they're both just trying to get the

127
00:04:36,639 --> 00:04:43,360
first thing to work. Um, so the only the

128
00:04:41,199 --> 00:04:45,680
only advice I have about trying to start

129
00:04:43,360 --> 00:04:47,759
something big is pick pick a market

130
00:04:45,680 --> 00:04:49,280
where it seems like there's some version

131
00:04:47,759 --> 00:04:51,280
of the future where it could be big if

132
00:04:49,280 --> 00:04:52,639
it works. But other than that, it's like

133
00:04:51,280 --> 00:04:54,960
one dumb foot in front of the other for

134
00:04:52,639 --> 00:04:56,639
a long time. How people use chatd has

135
00:04:54,960 --> 00:04:59,840
changed a lot. How people use your API

136
00:04:56,639 --> 00:05:01,919
has changed a lot. Uh, what surprises

137
00:04:59,840 --> 00:05:04,639
you the most with the latest models like

138
00:05:01,919 --> 00:05:06,560
03 and what emergent behaviors or use

139
00:05:04,639 --> 00:05:08,400
cases are standing out to you right now?

140
00:05:06,560 --> 00:05:09,840
I think we're in a really interesting

141
00:05:08,400 --> 00:05:12,080
time. We haven't been in one of these

142
00:05:09,840 --> 00:05:13,759
for a while. Uh but like right now we're

143
00:05:12,080 --> 00:05:16,320
in an interesting time where the the

144
00:05:13,759 --> 00:05:19,120
product overhang relative to what the

145
00:05:16,320 --> 00:05:22,000
models like what the models are capable

146
00:05:19,120 --> 00:05:23,840
of is here the products that people have

147
00:05:22,000 --> 00:05:26,320
figured out to build is way down here.

148
00:05:23,840 --> 00:05:28,400
There's a huge even if the models got no

149
00:05:26,320 --> 00:05:29,840
better which of course they will uh

150
00:05:28,400 --> 00:05:33,039
there's a huge amount of new stuff to

151
00:05:29,840 --> 00:05:36,080
build. And also like last week 03 cost

152
00:05:33,039 --> 00:05:37,919
five times as much as it did this week

153
00:05:36,080 --> 00:05:39,520
and that's going to keep going. I think

154
00:05:37,919 --> 00:05:42,960
people will be astonished at how much

155
00:05:39,520 --> 00:05:44,240
the price per performance falls. Um we

156
00:05:42,960 --> 00:05:45,520
have an open source model coming out

157
00:05:44,240 --> 00:05:48,520
soon. I think people are going to be

158
00:05:45,520 --> 00:05:48,520
Yeah,

159
00:05:48,720 --> 00:05:51,759
I don't want to like steal the team's

160
00:05:50,320 --> 00:05:53,759
glory and pre-announce this, but I think

161
00:05:51,759 --> 00:05:55,360
you all will be astonished. I think it

162
00:05:53,759 --> 00:05:58,000
will be like much better than you're

163
00:05:55,360 --> 00:06:00,800
hoping for and the ability to like use

164
00:05:58,000 --> 00:06:02,560
it to run incredibly powerful models

165
00:06:00,800 --> 00:06:05,280
locally is gonna like really really

166
00:06:02,560 --> 00:06:08,720
surprise people on what's possible. Um

167
00:06:05,280 --> 00:06:11,199
but so you have this world where like

168
00:06:08,720 --> 00:06:14,000
the model capability has gone into like

169
00:06:11,199 --> 00:06:16,000
a kind of new like a very new realm. Um

170
00:06:14,000 --> 00:06:17,759
the cost of the APIs are going to keep

171
00:06:16,000 --> 00:06:19,120
falling quite dramatically. the open

172
00:06:17,759 --> 00:06:21,039
source models are going to be super

173
00:06:19,120 --> 00:06:22,960
great and I think we have not yet seen

174
00:06:21,039 --> 00:06:24,479
the level of new product innovation that

175
00:06:22,960 --> 00:06:26,560
the reasoning models are capable of

176
00:06:24,479 --> 00:06:29,039
which makes sense they're pretty new but

177
00:06:26,560 --> 00:06:31,840
this is like an exceptional time to go

178
00:06:29,039 --> 00:06:34,479
build a company that takes advantage of

179
00:06:31,840 --> 00:06:35,919
this sort of new thing that exists this

180
00:06:34,479 --> 00:06:39,280
sort of new square on a periodic table

181
00:06:35,919 --> 00:06:40,960
that no one has built with yet. So only

182
00:06:39,280 --> 00:06:42,720
in the last month I think have we really

183
00:06:40,960 --> 00:06:45,360
started to see startups that are saying

184
00:06:42,720 --> 00:06:47,039
okay like reasoning models are different

185
00:06:45,360 --> 00:06:48,800
you know the whole interaction model is

186
00:06:47,039 --> 00:06:51,039
different and really building for that.

187
00:06:48,800 --> 00:06:53,919
I mean for me even memory has turned

188
00:06:51,039 --> 00:06:55,360
into it feels like I'm talking to

189
00:06:53,919 --> 00:06:57,199
someone who knows me which is

190
00:06:55,360 --> 00:06:58,800
interesting. Yeah me memory is my

191
00:06:57,199 --> 00:07:00,160
favorite feature that we've launched

192
00:06:58,800 --> 00:07:01,520
this year. I don't think most people at

193
00:07:00,160 --> 00:07:03,280
open air would say that because and

194
00:07:01,520 --> 00:07:06,960
we've launched a lot of stuff but I love

195
00:07:03,280 --> 00:07:08,639
memory and chatbt. Um and I think it

196
00:07:06,960 --> 00:07:11,039
points to where we will hopefully go

197
00:07:08,639 --> 00:07:14,000
with the product which is you will have

198
00:07:11,039 --> 00:07:15,919
this like entity that gets to know you

199
00:07:14,000 --> 00:07:18,240
that connects to all your stuff and that

200
00:07:15,919 --> 00:07:19,759
is like proactively helping you. It'll

201
00:07:18,240 --> 00:07:21,039
it won't just be like you send a message

202
00:07:19,759 --> 00:07:22,400
and it sends you one back. But it'll

203
00:07:21,039 --> 00:07:23,520
like be running all the time. It'll be

204
00:07:22,400 --> 00:07:24,800
looking at your stuff. It'll know when

205
00:07:23,520 --> 00:07:26,400
to send you a message. It'll know when

206
00:07:24,800 --> 00:07:28,080
to go do something on your behalf.

207
00:07:26,400 --> 00:07:30,160
you'll have, you know, special new

208
00:07:28,080 --> 00:07:31,360
devices and it'll be integrated on every

209
00:07:30,160 --> 00:07:32,720
other service you use and you'll just

210
00:07:31,360 --> 00:07:34,560
have this thing running with you

211
00:07:32,720 --> 00:07:36,080
throughout your life. I think memory is

212
00:07:34,560 --> 00:07:39,360
the first time where people can sort of

213
00:07:36,080 --> 00:07:42,240
see that coming. Uh, back in the day you

214
00:07:39,360 --> 00:07:43,520
tweeted a little bit about uh her where,

215
00:07:42,240 --> 00:07:45,919
you know, when is that coming? Can you

216
00:07:43,520 --> 00:07:48,080
give us an alpha leak around that? I

217
00:07:45,919 --> 00:07:49,440
think gradually is the answer. No, no.

218
00:07:48,080 --> 00:07:51,199
If I had a date in mind, I would

219
00:07:49,440 --> 00:07:54,720
probably just be excited and tell you.

220
00:07:51,199 --> 00:07:56,800
Um, but like it's a little bit here with

221
00:07:54,720 --> 00:07:58,240
memory, right? It'll be a little more

222
00:07:56,800 --> 00:07:59,440
here when it's persistently running in

223
00:07:58,240 --> 00:08:00,879
the background and sending you stuff.

224
00:07:59,440 --> 00:08:03,440
It'll be a lot more here when we ship

225
00:08:00,879 --> 00:08:05,599
the first new device. Um, but I think

226
00:08:03,440 --> 00:08:08,479
the key of her is not the little piece

227
00:08:05,599 --> 00:08:10,160
of hardware. It's that this thing

228
00:08:08,479 --> 00:08:11,840
got to a point where it could run in the

229
00:08:10,160 --> 00:08:14,240
background and feel like a sort of AI

230
00:08:11,840 --> 00:08:16,240
companion. I guess we're starting to see

231
00:08:14,240 --> 00:08:19,440
um the power of LMS with integrations

232
00:08:16,240 --> 00:08:22,000
into your real data. Uh, you know, I've

233
00:08:19,440 --> 00:08:24,479
heard rumors that MCP is coming to

234
00:08:22,000 --> 00:08:28,199
OpenAI. I think today. Yeah. Oh, today.

235
00:08:24,479 --> 00:08:28,199
I think so. Fantastic.

236
00:08:29,520 --> 00:08:32,640
What has been surprising about the

237
00:08:31,039 --> 00:08:35,200
actual integrations? Like have you been

238
00:08:32,640 --> 00:08:37,599
seeing people actually operating on

239
00:08:35,200 --> 00:08:39,440
their core database? You know, at YC we

240
00:08:37,599 --> 00:08:41,680
actually have that agent infrastructure

241
00:08:39,440 --> 00:08:43,360
internally and we use it all the time.

242
00:08:41,680 --> 00:08:45,680
Definitely people are starting to use

243
00:08:43,360 --> 00:08:47,760
chatbt as this like operating system

244
00:08:45,680 --> 00:08:50,560
with everything with their whole lives

245
00:08:47,760 --> 00:08:52,959
in it. Um and integrating into as many

246
00:08:50,560 --> 00:08:54,560
data sources as possible is important.

247
00:08:52,959 --> 00:08:56,880
um devices that are always with you,

248
00:08:54,560 --> 00:08:58,800
like new kinds of web browsers, the

249
00:08:56,880 --> 00:09:00,160
connection to all data sources, memory,

250
00:08:58,800 --> 00:09:02,320
and then a model that's persistently

251
00:09:00,160 --> 00:09:03,600
running. You put all that together, I I

252
00:09:02,320 --> 00:09:05,440
think you get to like a pretty powerful

253
00:09:03,600 --> 00:09:07,040
place. Do you think that'll be in the

254
00:09:05,440 --> 00:09:08,880
cloud in the future, or will it be on

255
00:09:07,040 --> 00:09:10,640
our desktop or some mix of both? Some

256
00:09:08,880 --> 00:09:13,120
mix of all of that. Definitely people

257
00:09:10,640 --> 00:09:15,519
will run local models for some things

258
00:09:13,120 --> 00:09:17,600
like man, if we could push like half the

259
00:09:15,519 --> 00:09:19,519
chatbt workload onto your local devices,

260
00:09:17,600 --> 00:09:22,399
no one would be happier than us. like

261
00:09:19,519 --> 00:09:24,480
our cloud. We I think we will run the

262
00:09:22,399 --> 00:09:26,080
like largest and most expensive piece of

263
00:09:24,480 --> 00:09:27,200
infrastructure in the world pretty soon.

264
00:09:26,080 --> 00:09:28,720
So, if we could push some of that off,

265
00:09:27,200 --> 00:09:30,800
that'd be great. Um, but a lot of it

266
00:09:28,720 --> 00:09:33,279
will run on the cloud. Is it surprising

267
00:09:30,800 --> 00:09:35,120
to you how hard it is to get compute? I

268
00:09:33,279 --> 00:09:38,399
mean, we've gotten really good at it,

269
00:09:35,120 --> 00:09:41,600
but it is

270
00:09:38,399 --> 00:09:44,560
we we went from like a

271
00:09:41,600 --> 00:09:46,160
no a zero like no chatbt.com didn't

272
00:09:44,560 --> 00:09:47,600
exist two and a half years ago to like

273
00:09:46,160 --> 00:09:49,200
the fifth biggest website in the world.

274
00:09:47,600 --> 00:09:50,720
It'll be the third at some point,

275
00:09:49,200 --> 00:09:53,519
hopefully someday the first if our

276
00:09:50,720 --> 00:09:55,120
current growth rates continue. And I

277
00:09:53,519 --> 00:09:58,240
think doing that is just hard no matter

278
00:09:55,120 --> 00:10:01,440
what. Like that's you usually get longer

279
00:09:58,240 --> 00:10:05,920
than we've gotten to to scale up a like

280
00:10:01,440 --> 00:10:06,880
infrastructure to for a new company. But

281
00:10:05,920 --> 00:10:09,440
you know, there's like a lot of people

282
00:10:06,880 --> 00:10:10,800
that want to help. Well, it's incredible

283
00:10:09,440 --> 00:10:13,440
incredible work that you guys have been

284
00:10:10,800 --> 00:10:16,079
doing. Um, we're seeing reasoning models

285
00:10:13,440 --> 00:10:19,839
like 03 and 04 mini uh evolve in

286
00:10:16,079 --> 00:10:21,440
parallel with multimodal models like 40.

287
00:10:19,839 --> 00:10:23,839
What happens when these two threads

288
00:10:21,440 --> 00:10:26,320
converge? And what's the vision for GPT5

289
00:10:23,839 --> 00:10:28,240
and beyond? I mean, we we we won't get

290
00:10:26,320 --> 00:10:30,079
all the way here with GPT5, but

291
00:10:28,240 --> 00:10:32,880
eventually we do want one integrated

292
00:10:30,079 --> 00:10:34,959
model that can like reason when it needs

293
00:10:32,880 --> 00:10:37,279
to and generate like real-time video

294
00:10:34,959 --> 00:10:39,200
when it needs to do that. If you ask a

295
00:10:37,279 --> 00:10:41,200
question, you could imagine it thinking

296
00:10:39,200 --> 00:10:43,600
super hard, doing some research, writing

297
00:10:41,200 --> 00:10:45,839
a bunch of code just in time for like a

298
00:10:43,600 --> 00:10:47,680
brand new app only for you to use or

299
00:10:45,839 --> 00:10:49,839
kind of like rendering live video that

300
00:10:47,680 --> 00:10:51,920
you can interact with. Um, so I think

301
00:10:49,839 --> 00:10:54,880
that will feel like

302
00:10:51,920 --> 00:10:57,120
a real new kind of computer interface.

303
00:10:54,880 --> 00:10:59,440
that AI sort of somewhat already does,

304
00:10:57,120 --> 00:11:01,440
but when we get to a model that has like

305
00:10:59,440 --> 00:11:03,040
true complete multimodality, like

306
00:11:01,440 --> 00:11:06,000
perfect video, perfect coding,

307
00:11:03,040 --> 00:11:08,079
everything and deep reasoning, that will

308
00:11:06,000 --> 00:11:10,399
that will feel like quite powerful. It

309
00:11:08,079 --> 00:11:12,720
seems like that might be a hop step over

310
00:11:10,399 --> 00:11:16,079
to the uh embodied aspect. You know,

311
00:11:12,720 --> 00:11:19,519
that's having vision, having speech, and

312
00:11:16,079 --> 00:11:21,760
having reasoning is a hop step to, you

313
00:11:19,519 --> 00:11:24,320
know, yeah, basically the robot we want.

314
00:11:21,760 --> 00:11:27,120
our our our strategy has been nail that

315
00:11:24,320 --> 00:11:28,959
first um and then make sure we can

316
00:11:27,120 --> 00:11:31,279
connect that to a robot. But the time

317
00:11:28,959 --> 00:11:34,240
for the robot is coming soon. Um I think

318
00:11:31,279 --> 00:11:36,000
I am very excited about a world where

319
00:11:34,240 --> 00:11:37,920
when you sign up for like the highest

320
00:11:36,000 --> 00:11:39,680
tier of the chatbt subscription, we send

321
00:11:37,920 --> 00:11:41,839
you a free humanoid robot too. I mean

322
00:11:39,680 --> 00:11:44,720
that future is going to be pretty wild

323
00:11:41,839 --> 00:11:47,279
being able to have robots that do real

324
00:11:44,720 --> 00:11:51,120
work in the real world. I think we're

325
00:11:47,279 --> 00:11:53,279
not that far away now. uh the mechanical

326
00:11:51,120 --> 00:11:56,079
engineering of robots is been quite

327
00:11:53,279 --> 00:11:57,360
difficult uh and and the sort of AI for

328
00:11:56,079 --> 00:12:00,000
the cognitive part has been quite

329
00:11:57,360 --> 00:12:02,480
difficult too but it feels within grasp

330
00:12:00,000 --> 00:12:04,560
um and I think in a few years robots

331
00:12:02,480 --> 00:12:06,720
will

332
00:12:04,560 --> 00:12:08,560
start to do super useful stuff making a

333
00:12:06,720 --> 00:12:10,480
billion robots is still going to take a

334
00:12:08,560 --> 00:12:12,000
while but I don't know I'm interested in

335
00:12:10,480 --> 00:12:13,600
the question of how many robots do you

336
00:12:12,000 --> 00:12:15,120
need to fully automate the supply chain

337
00:12:13,600 --> 00:12:16,959
like if you make a million humanoid

338
00:12:15,120 --> 00:12:18,639
robots the oldfashioned way can they run

339
00:12:16,959 --> 00:12:20,480
the entire supply chain

340
00:12:18,639 --> 00:12:23,200
drive the mining equipment like drive

341
00:12:20,480 --> 00:12:25,519
the container ships run the you know

342
00:12:23,200 --> 00:12:27,760
foundaries and make the new robots and

343
00:12:25,519 --> 00:12:29,760
then maybe like you actually can get a

344
00:12:27,760 --> 00:12:31,519
lot of robots in the world quickly but

345
00:12:29,760 --> 00:12:33,040
the demand for human robots in the world

346
00:12:31,519 --> 00:12:34,240
will be far more than we know how to

347
00:12:33,040 --> 00:12:36,399
think about with the current supply

348
00:12:34,240 --> 00:12:38,320
chain. I guess uh when you were sitting

349
00:12:36,399 --> 00:12:40,079
in my seat, one of the things you led

350
00:12:38,320 --> 00:12:44,000
was uh you know a lot more investment

351
00:12:40,079 --> 00:12:46,079
into hard tech at YC. Um sitting here

352
00:12:44,000 --> 00:12:48,320
where we are geopolitically,

353
00:12:46,079 --> 00:12:50,880
you know, what what do we need to do to

354
00:12:48,320 --> 00:12:52,720
make sure that America can actually have

355
00:12:50,880 --> 00:12:54,399
manufacturing in industrial capacity,

356
00:12:52,720 --> 00:12:56,800
you know, we can't even build precision

357
00:12:54,399 --> 00:12:58,959
screws and large sheet metal without

358
00:12:56,800 --> 00:13:00,240
crazy cost overruns. You know, how what

359
00:12:58,959 --> 00:13:02,399
can we do to make sure that happens

360
00:13:00,240 --> 00:13:04,639
here? There are all of these answers

361
00:13:02,399 --> 00:13:06,160
that people throw around and have thrown

362
00:13:04,639 --> 00:13:09,120
around the same things for a while and

363
00:13:06,160 --> 00:13:11,120
it clearly hasn't worked. So, uh I think

364
00:13:09,120 --> 00:13:12,560
all of the policy is worth trying, but

365
00:13:11,120 --> 00:13:14,160
my instinct is we need to try something

366
00:13:12,560 --> 00:13:16,959
new. We shouldn't keep trying the same

367
00:13:14,160 --> 00:13:20,399
failed stuff. And you know, like AI and

368
00:13:16,959 --> 00:13:22,399
robotics does give us a new possibility

369
00:13:20,399 --> 00:13:24,560
of a way to bring manufacturing back

370
00:13:22,399 --> 00:13:26,399
here and to bring sort of these complex

371
00:13:24,560 --> 00:13:28,480
industries here in a really important

372
00:13:26,399 --> 00:13:30,480
new way. And I would say that's at least

373
00:13:28,480 --> 00:13:32,880
worth trying. Yeah.

374
00:13:30,480 --> 00:13:34,480
um what does defensibility look like

375
00:13:32,880 --> 00:13:37,440
here? You know, one of the classic

376
00:13:34,480 --> 00:13:39,040
questions is, you know, how do I uh

377
00:13:37,440 --> 00:13:40,800
start a startup that doesn't get run

378
00:13:39,040 --> 00:13:44,000
over by Open AI? That's sort of the

379
00:13:40,800 --> 00:13:48,200
number one question that's in our chat.

380
00:13:44,000 --> 00:13:48,200
We don't want to run you over. Um

381
00:13:48,639 --> 00:13:52,480
look, we're going to do our thing

382
00:13:51,040 --> 00:13:55,200
hopefully very well. We are going to try

383
00:13:52,480 --> 00:13:57,600
to make the best super assistant out of

384
00:13:55,200 --> 00:13:59,120
Chat GBT that we can. we're gonna add

385
00:13:57,600 --> 00:14:03,760
the things that we think we need to add

386
00:13:59,120 --> 00:14:05,519
to that. Um, but that is like one small

387
00:14:03,760 --> 00:14:07,680
part of the opportunity in front of us.

388
00:14:05,519 --> 00:14:08,959
And it makes us sad when people are

389
00:14:07,680 --> 00:14:11,120
like, I'm going to start a new startup

390
00:14:08,959 --> 00:14:12,720
and I'm going to like make a version of

391
00:14:11,120 --> 00:14:14,320
Chaz GBT because we think we're going to

392
00:14:12,720 --> 00:14:18,560
do that pretty well and we have like,

393
00:14:14,320 --> 00:14:20,480
you know, kind of a big head start, but

394
00:14:18,560 --> 00:14:22,160
there is so much more space to go after

395
00:14:20,480 --> 00:14:24,560
and there are so many incredible other

396
00:14:22,160 --> 00:14:27,360
companies that have been built using our

397
00:14:24,560 --> 00:14:29,040
platform. Um, we would like to make it

398
00:14:27,360 --> 00:14:31,519
easier for you all. We would like to do

399
00:14:29,040 --> 00:14:33,600
more things like finally now you can

400
00:14:31,519 --> 00:14:35,440
imagine that chat GPT could drive a lot

401
00:14:33,600 --> 00:14:37,120
of traffic to new startups and that

402
00:14:35,440 --> 00:14:38,320
there's like a new kind of app or agent

403
00:14:37,120 --> 00:14:40,639
or whatever you want to call it store

404
00:14:38,320 --> 00:14:42,639
that we could do inside of ChatgPT and

405
00:14:40,639 --> 00:14:43,920
drive traffic to new startups to help.

406
00:14:42,639 --> 00:14:45,600
You could imagine that we could do like

407
00:14:43,920 --> 00:14:47,360
a signin with OpenAI and people could

408
00:14:45,600 --> 00:14:48,800
bring their, you know, personalized

409
00:14:47,360 --> 00:14:50,160
model and easily connect it to a new

410
00:14:48,800 --> 00:14:53,519
startup and that would probably help in

411
00:14:50,160 --> 00:14:54,720
a bunch of ways. Um so we want to be a

412
00:14:53,519 --> 00:14:57,360
platform for other people to build

413
00:14:54,720 --> 00:14:59,120
stuff. Our advice is like don't build

414
00:14:57,360 --> 00:15:03,040
our core

415
00:14:59,120 --> 00:15:06,880
you know chat assistant. Um but there is

416
00:15:03,040 --> 00:15:08,800
another problem which is

417
00:15:06,880 --> 00:15:10,320
and this is the same for every every

418
00:15:08,800 --> 00:15:12,639
kind of like moment that I've seen in

419
00:15:10,320 --> 00:15:14,639
startup history. People get excited

420
00:15:12,639 --> 00:15:16,560
about the same thing at the same time.

421
00:15:14,639 --> 00:15:18,399
And so rather than go build the thing

422
00:15:16,560 --> 00:15:19,920
that you have thought of that is not

423
00:15:18,399 --> 00:15:22,959
everybody what else is doing, we are

424
00:15:19,920 --> 00:15:24,399
like very social creatures and we get

425
00:15:22,959 --> 00:15:28,720
very influenced by what other people are

426
00:15:24,399 --> 00:15:31,120
doing. And I bet if Gary listed off the

427
00:15:28,720 --> 00:15:33,040
five ideas that he hears most often of

428
00:15:31,120 --> 00:15:34,320
what people want to build with AI, like

429
00:15:33,040 --> 00:15:37,760
half the room would raise their hands

430
00:15:34,320 --> 00:15:40,399
for working on one of those five. and

431
00:15:37,760 --> 00:15:42,320
and the I there is hopefully in this

432
00:15:40,399 --> 00:15:44,000
room um the person who's going to start

433
00:15:42,320 --> 00:15:47,040
a company that is like much bigger than

434
00:15:44,000 --> 00:15:48,320
OpenAI someday and I would bet that

435
00:15:47,040 --> 00:15:52,560
person is not working on any of the

436
00:15:48,320 --> 00:15:54,560
five. So it is hard to build something

437
00:15:52,560 --> 00:15:55,680
defensible if everybody else is trying

438
00:15:54,560 --> 00:15:58,000
to do the same thing. Sometimes it

439
00:15:55,680 --> 00:16:00,240
works. It's not impossible. But the best

440
00:15:58,000 --> 00:16:01,920
the most enduring companies are usually

441
00:16:00,240 --> 00:16:05,040
not doing the same thing as everybody

442
00:16:01,920 --> 00:16:06,639
else. And that gives you time to figure

443
00:16:05,040 --> 00:16:08,399
out what the great product is, how to

444
00:16:06,639 --> 00:16:10,399
build the technology before you have to

445
00:16:08,399 --> 00:16:12,079
answer the defensibility question. It

446
00:16:10,399 --> 00:16:13,360
took us a long time to figure out how to

447
00:16:12,079 --> 00:16:15,920
answer the defensibility question for

448
00:16:13,360 --> 00:16:17,839
CHBT. Um, we had built this thing and

449
00:16:15,920 --> 00:16:20,320
for a long time the only defensibility

450
00:16:17,839 --> 00:16:23,440
was like we had the only product out in

451
00:16:20,320 --> 00:16:25,839
the market. Um, and then we kind of like

452
00:16:23,440 --> 00:16:27,360
be a brand that started to be wellknown

453
00:16:25,839 --> 00:16:28,399
and now we have things like memory and

454
00:16:27,360 --> 00:16:31,519
connections, a whole bunch of other

455
00:16:28,399 --> 00:16:32,880
stuff that really is defensible. But you

456
00:16:31,519 --> 00:16:35,199
know that was like a fair criticism of

457
00:16:32,880 --> 00:16:37,920
us for a long time. We didn't have any

458
00:16:35,199 --> 00:16:40,320
defensibility strategy. We just like had

459
00:16:37,920 --> 00:16:42,000
the only good thing out there and then

460
00:16:40,320 --> 00:16:43,759
you have some window before which you

461
00:16:42,000 --> 00:16:45,279
have to build defensibility. One of the

462
00:16:43,759 --> 00:16:48,399
things we've talked about in the past is

463
00:16:45,279 --> 00:16:51,680
that uh we both are big followers of uh

464
00:16:48,399 --> 00:16:55,759
Peter Teal in that he talks a lot about

465
00:16:51,680 --> 00:16:58,079
being contrarian but right. Um

466
00:16:55,759 --> 00:17:00,160
I think that you've Peter is a genius.

467
00:16:58,079 --> 00:17:02,160
Absolutely. And you found you've been

468
00:17:00,160 --> 00:17:03,839
contrarian in really fundamental ways. I

469
00:17:02,160 --> 00:17:05,679
mean going back you know to the

470
00:17:03,839 --> 00:17:08,000
beginning of the conversation people

471
00:17:05,679 --> 00:17:10,000
thought oh this idea that the scaling

472
00:17:08,000 --> 00:17:13,039
laws are valuable today it's you know

473
00:17:10,000 --> 00:17:15,839
taken as basic truth but it was exactly

474
00:17:13,039 --> 00:17:18,480
the opposite of ground truth not that

475
00:17:15,839 --> 00:17:20,480
many years ago when you got that push

476
00:17:18,480 --> 00:17:23,760
back you know what did you and your team

477
00:17:20,480 --> 00:17:25,760
feel did you say you know fui I won't do

478
00:17:23,760 --> 00:17:28,079
what you tell me you know I'm going I'm

479
00:17:25,760 --> 00:17:30,240
going to push back against you know

480
00:17:28,079 --> 00:17:32,320
getting push back means that this is a

481
00:17:30,240 --> 00:17:34,480
contrarian area and we're going to bet

482
00:17:32,320 --> 00:17:37,919
here and we're going to be right. It is

483
00:17:34,480 --> 00:17:39,360
hard to have conviction in the face of a

484
00:17:37,919 --> 00:17:41,919
lot of other people telling you you're

485
00:17:39,360 --> 00:17:44,880
wrong. And I think people who don't who

486
00:17:41,919 --> 00:17:48,799
say it's easy are not being honest. It

487
00:17:44,880 --> 00:17:50,880
gets easier over time. Um but like I

488
00:17:48,799 --> 00:17:53,679
remember one time I can say this one

489
00:17:50,880 --> 00:17:56,080
because it got publicized um in early

490
00:17:53,679 --> 00:17:57,760
not early a few years into OpenAI where

491
00:17:56,080 --> 00:17:58,720
Elon sent us this really mean email.

492
00:17:57,760 --> 00:18:00,320
we've been working together for a while

493
00:17:58,720 --> 00:18:02,799
and said we had a zero% chance of

494
00:18:00,320 --> 00:18:04,559
success. Like not zero. Zero that we

495
00:18:02,799 --> 00:18:06,320
were totally failing. We had showed him

496
00:18:04,559 --> 00:18:08,080
like GPT1 recently. He was like, "This

497
00:18:06,320 --> 00:18:10,480
is crap. It's not gonna work. Doesn't

498
00:18:08,080 --> 00:18:12,720
make sense." And I was really a hero of

499
00:18:10,480 --> 00:18:14,799
mine at the time. And I remember going

500
00:18:12,720 --> 00:18:16,240
home that night and being like, "What if

501
00:18:14,799 --> 00:18:17,679
he's right? Like this sucks."

502
00:18:16,240 --> 00:18:18,720
You know, you're working so hard on this

503
00:18:17,679 --> 00:18:20,799
thing. Like you're pouring your life

504
00:18:18,720 --> 00:18:22,640
force into it. And you have these people

505
00:18:20,799 --> 00:18:25,360
who are smart and that you look up to

506
00:18:22,640 --> 00:18:28,400
and they say, "You are totally wrong."

507
00:18:25,360 --> 00:18:30,320
um or you know

508
00:18:28,400 --> 00:18:31,760
this is just not never going to work or

509
00:18:30,320 --> 00:18:32,640
you don't have defensibility, someone's

510
00:18:31,760 --> 00:18:35,760
going to kill you, this is going to

511
00:18:32,640 --> 00:18:37,919
happen, that's going to happen. And

512
00:18:35,760 --> 00:18:40,240
I don't have a magic answer other than

513
00:18:37,919 --> 00:18:43,440
it's really tough and it gets

514
00:18:40,240 --> 00:18:46,400
significantly easier over time, but it's

515
00:18:43,440 --> 00:18:48,320
going to happen to all of you and you

516
00:18:46,400 --> 00:18:50,000
just like get knocked down and get back

517
00:18:48,320 --> 00:18:52,559
up and brush yourself off and try to

518
00:18:50,000 --> 00:18:55,600
keep going. Let's talk uh AI agents. you

519
00:18:52,559 --> 00:18:57,760
know, that's uh sort of level three AGI.

520
00:18:55,600 --> 00:19:00,240
Uh this is the year I think Greg

521
00:18:57,760 --> 00:19:03,919
Brockman talked about recently. This is

522
00:19:00,240 --> 00:19:05,440
the year of the agent. Um

523
00:19:03,919 --> 00:19:07,600
with tools like operator code

524
00:19:05,440 --> 00:19:10,480
interpreter, what kind of workflows do

525
00:19:07,600 --> 00:19:12,880
you think will disappear uh or appear

526
00:19:10,480 --> 00:19:14,640
that we just aren't ready for yet? For a

527
00:19:12,880 --> 00:19:16,240
long time, chatb was like a Google

528
00:19:14,640 --> 00:19:17,520
replacement. You could ask it something

529
00:19:16,240 --> 00:19:19,039
that was about as long as a Google

530
00:19:17,520 --> 00:19:20,240
query. you know, maybe like half an hour

531
00:19:19,039 --> 00:19:23,679
worth of Google queries it could

532
00:19:20,240 --> 00:19:25,840
assemble together. And that was still

533
00:19:23,679 --> 00:19:27,360
pretty good, but it didn't it it it

534
00:19:25,840 --> 00:19:29,039
still felt like a more advanced version

535
00:19:27,360 --> 00:19:30,400
of search.

536
00:19:29,039 --> 00:19:32,240
But now you start to see things where

537
00:19:30,400 --> 00:19:34,960
you can like really give a task to

538
00:19:32,240 --> 00:19:37,919
codeex for example um or to deep

539
00:19:34,960 --> 00:19:39,840
research and you have this thing go off

540
00:19:37,919 --> 00:19:41,760
and do a bunch of stuff and come back to

541
00:19:39,840 --> 00:19:43,840
you with like a proposal. It's it's like

542
00:19:41,760 --> 00:19:45,360
a very junior employee that can work on

543
00:19:43,840 --> 00:19:49,200
something for like a short period of

544
00:19:45,360 --> 00:19:52,000
time. And if you think about how much of

545
00:19:49,200 --> 00:19:54,080
the work that the world does is work

546
00:19:52,000 --> 00:19:56,080
that can be done in front of a computer

547
00:19:54,080 --> 00:19:58,240
in like few hour chunks where you then

548
00:19:56,080 --> 00:20:01,360
have someone say like okay that was good

549
00:19:58,240 --> 00:20:02,480
enough or not. It's quite a lot. So I I

550
00:20:01,360 --> 00:20:03,520
think this is going to go this is part

551
00:20:02,480 --> 00:20:05,760
of that overhang we were talking about

552
00:20:03,520 --> 00:20:08,160
earlier but I think this is going to go

553
00:20:05,760 --> 00:20:10,400
quite far and I think with current 03 to

554
00:20:08,160 --> 00:20:12,480
say nothing of our next model you can

555
00:20:10,400 --> 00:20:14,000
build a lot of experiences like this.

556
00:20:12,480 --> 00:20:16,320
How do you see the future of human

557
00:20:14,000 --> 00:20:18,480
computer interaction and interfaces and

558
00:20:16,320 --> 00:20:21,440
what are sort of the limitations of

559
00:20:18,480 --> 00:20:23,919
those interfaces that you know motivated

560
00:20:21,440 --> 00:20:27,440
you? You know, one of the things that I

561
00:20:23,919 --> 00:20:30,640
think sci-fi got right is

562
00:20:27,440 --> 00:20:33,440
the idea of the interface almost melts

563
00:20:30,640 --> 00:20:34,960
away. Like voice interfaces today we

564
00:20:33,440 --> 00:20:36,880
think of as something that is kind of

565
00:20:34,960 --> 00:20:38,880
sucky because they don't work that well.

566
00:20:36,880 --> 00:20:40,880
But in theory, if you could say to a

567
00:20:38,880 --> 00:20:42,240
computer, "This is exactly what I want

568
00:20:40,880 --> 00:20:43,919
to happen today. and if there's any

569
00:20:42,240 --> 00:20:45,760
changes, if like I'm delayed, if you

570
00:20:43,919 --> 00:20:47,360
know something happens, I trust you to

571
00:20:45,760 --> 00:20:48,880
like go off and do all those things, but

572
00:20:47,360 --> 00:20:50,720
like I don't want to be interrupted. I

573
00:20:48,880 --> 00:20:52,400
don't want to think about it. Um, and it

574
00:20:50,720 --> 00:20:53,919
just did it all and you trusted that it

575
00:20:52,400 --> 00:20:55,360
worked.

576
00:20:53,919 --> 00:20:56,799
That would be an interface that almost

577
00:20:55,360 --> 00:20:58,320
melted away except when it, you know,

578
00:20:56,799 --> 00:21:00,080
was like a super great human assistant

579
00:20:58,320 --> 00:21:02,000
needed to talk to you. But you would be

580
00:21:00,080 --> 00:21:05,039
like really thrilled. When I like use my

581
00:21:02,000 --> 00:21:07,120
phone today, I feel like I am like

582
00:21:05,039 --> 00:21:08,960
walking down Time Square in New York

583
00:21:07,120 --> 00:21:10,320
getting like bumped into by people. I

584
00:21:08,960 --> 00:21:11,679
love my phone. It's an incredible piece

585
00:21:10,320 --> 00:21:13,200
of technology, but it's like

586
00:21:11,679 --> 00:21:14,559
notification here, this thing happening,

587
00:21:13,200 --> 00:21:16,080
you know, this thing popping up, like

588
00:21:14,559 --> 00:21:18,799
bright colors, like all kinds of

589
00:21:16,080 --> 00:21:20,400
flashing things in me. It's just

590
00:21:18,799 --> 00:21:24,159
stressful.

591
00:21:20,400 --> 00:21:26,320
And I can imagine an interface where the

592
00:21:24,159 --> 00:21:28,640
computer mostly melts away. It does the

593
00:21:26,320 --> 00:21:30,000
stuff I need. Um, but I really trust

594
00:21:28,640 --> 00:21:32,480
that it's going to do a great job of

595
00:21:30,000 --> 00:21:33,840
like surfacing information to me, making

596
00:21:32,480 --> 00:21:36,320
judgment calls about when not to, acting

597
00:21:33,840 --> 00:21:38,880
on my behalf when it should. Uh, and I'm

598
00:21:36,320 --> 00:21:39,840
I'm quite excited for that. Uh, I'm not

599
00:21:38,880 --> 00:21:40,880
going to like tell you what the new

600
00:21:39,840 --> 00:21:42,400
device is. Well, I'll tell you like

601
00:21:40,880 --> 00:21:46,400
one-on-one, but I'm not going to tell

602
00:21:42,400 --> 00:21:48,400
the room, but it's very like I I hope I

603
00:21:46,400 --> 00:21:51,679
hope we can sort of like show people a

604
00:21:48,400 --> 00:21:53,280
different way to have computers. Is that

605
00:21:51,679 --> 00:21:55,280
one of the reasons why you brought on

606
00:21:53,280 --> 00:21:58,080
one of the greatest living designers on

607
00:21:55,280 --> 00:22:01,440
the planet, John, in Johnny Ivan, IO?

608
00:21:58,080 --> 00:22:03,600
Yeah, he he is he is amazing. Um, he is

609
00:22:01,440 --> 00:22:05,600
he really lives up to all the hype. I I

610
00:22:03,600 --> 00:22:07,760
think we've only had kind of two big

611
00:22:05,600 --> 00:22:09,520
revolutions in computer interfaces

612
00:22:07,760 --> 00:22:11,360
really in the last like 50 years. So we

613
00:22:09,520 --> 00:22:13,520
had the you know keyboard and mouse and

614
00:22:11,360 --> 00:22:16,320
screen and then we had touch and phones

615
00:22:13,520 --> 00:22:18,159
and the opportunity to do a new one

616
00:22:16,320 --> 00:22:21,440
doesn't come along that often and I

617
00:22:18,159 --> 00:22:22,720
think AI really does totally open the

618
00:22:21,440 --> 00:22:24,480
playing field for something completely

619
00:22:22,720 --> 00:22:26,159
new and I think if you got to pick one

620
00:22:24,480 --> 00:22:28,960
person to bet on to figure that out he

621
00:22:26,159 --> 00:22:31,120
is the obvious bet. Yeah. So one of the

622
00:22:28,960 --> 00:22:33,679
things that uh we've been debating at YC

623
00:22:31,120 --> 00:22:35,760
that you know don't know if this is good

624
00:22:33,679 --> 00:22:38,720
might be scary for a lot of software

625
00:22:35,760 --> 00:22:41,120
engineers who want to create B2B SAS is

626
00:22:38,720 --> 00:22:42,799
this idea that uh what if in the future

627
00:22:41,120 --> 00:22:45,760
you had your you know underlying

628
00:22:42,799 --> 00:22:47,840
database you have an API layer that is

629
00:22:45,760 --> 00:22:50,240
uh you know your access control and

630
00:22:47,840 --> 00:22:52,640
enforces your business logic and then

631
00:22:50,240 --> 00:22:56,000
the interface is the LLM like your

632
00:22:52,640 --> 00:22:58,240
computer is literally you know the agent

633
00:22:56,000 --> 00:23:00,640
and uh you have just in time software.

634
00:22:58,240 --> 00:23:02,559
They're like complex flows. You're just

635
00:23:00,640 --> 00:23:04,799
going to go in and it'll code gen an

636
00:23:02,559 --> 00:23:06,720
artifact or, you know, a pain for you

637
00:23:04,799 --> 00:23:08,880
that like does that thing you wanted and

638
00:23:06,720 --> 00:23:10,640
it'll go in the file and it'll bring it

639
00:23:08,880 --> 00:23:13,679
back if you ever need it. That's going

640
00:23:10,640 --> 00:23:15,200
to happen. Yeah. Look here here there

641
00:23:13,679 --> 00:23:17,120
are two ways you can look at this. Uh

642
00:23:15,200 --> 00:23:18,320
first of all, assume you all are like

643
00:23:17,120 --> 00:23:19,600
starting startups or have started

644
00:23:18,320 --> 00:23:21,760
startups, think about starting startups.

645
00:23:19,600 --> 00:23:23,919
This is the best time ever in

646
00:23:21,760 --> 00:23:29,480
the history of technology ever, period,

647
00:23:23,919 --> 00:23:29,480
to start a company. Um yeah, this is

648
00:23:30,960 --> 00:23:36,159
uh and but part of the reason it's the

649
00:23:33,360 --> 00:23:38,320
best is because like the ground is

650
00:23:36,159 --> 00:23:40,240
shaking and it's true there are a lot of

651
00:23:38,320 --> 00:23:41,840
these challenges. So on one hand you can

652
00:23:40,240 --> 00:23:44,720
look at something like that and say we

653
00:23:41,840 --> 00:23:46,960
have been a you know SAS company and now

654
00:23:44,720 --> 00:23:48,480
like all of the code can just be

655
00:23:46,960 --> 00:23:50,240
generated right in time when someone

656
00:23:48,480 --> 00:23:52,960
needs it and what does that mean for us

657
00:23:50,240 --> 00:23:54,320
or you can look at it and say wow this

658
00:23:52,960 --> 00:23:56,480
is going to happen but it's going to

659
00:23:54,320 --> 00:23:59,440
happen to everybody and the way startups

660
00:23:56,480 --> 00:24:02,320
win is

661
00:23:59,440 --> 00:24:05,360
when they can iterate faster than big

662
00:24:02,320 --> 00:24:07,760
companies and they can do it at a much

663
00:24:05,360 --> 00:24:09,200
lower cost. like big companies have a

664
00:24:07,760 --> 00:24:12,240
lot of advantages but they iterate very

665
00:24:09,200 --> 00:24:14,320
slowly um and

666
00:24:12,240 --> 00:24:16,080
they you know if something is like very

667
00:24:14,320 --> 00:24:19,919
cheap then a lot of their big advantages

668
00:24:16,080 --> 00:24:22,640
go away. So you can look at this all of

669
00:24:19,919 --> 00:24:24,080
these problems one way or another. But

670
00:24:22,640 --> 00:24:27,039
the way I would recommend looking at

671
00:24:24,080 --> 00:24:29,120
them is everybody is going to face the

672
00:24:27,039 --> 00:24:32,159
same challenges and opportunities. But

673
00:24:29,120 --> 00:24:34,559
when the clock cycle of the industry

674
00:24:32,159 --> 00:24:36,320
changes this much startups almost always

675
00:24:34,559 --> 00:24:37,919
win and we've probably never seen it

676
00:24:36,320 --> 00:24:38,960
change this much. Act on it from that

677
00:24:37,919 --> 00:24:40,640
direction. I think you'll be in

678
00:24:38,960 --> 00:24:43,039
incredible shape. maybe you can invite

679
00:24:40,640 --> 00:24:44,960
me sometime to do a talk about like what

680
00:24:43,039 --> 00:24:46,240
the areas of defensibility that you can

681
00:24:44,960 --> 00:24:47,600
build over time are because I think that

682
00:24:46,240 --> 00:24:49,440
is the inherent question. people are

683
00:24:47,600 --> 00:24:50,640
like, "Oh, okay. You know, I'm a SAS

684
00:24:49,440 --> 00:24:52,000
company. There's going to be just in

685
00:24:50,640 --> 00:24:54,080
time software." I think the question

686
00:24:52,000 --> 00:24:56,000
behind the question is like, "What are

687
00:24:54,080 --> 00:24:57,600
actual defensibility strategies?" So,

688
00:24:56,000 --> 00:24:59,440
that would be a fun talk someday, I

689
00:24:57,600 --> 00:25:02,159
guess. Uh you know, backstage at one of

690
00:24:59,440 --> 00:25:03,840
the last events we had uh you know, we

691
00:25:02,159 --> 00:25:05,360
were talking about the this there's this

692
00:25:03,840 --> 00:25:08,559
book that's sort of like the classic

693
00:25:05,360 --> 00:25:10,480
Mckenzie, which is the seven powers. And

694
00:25:08,559 --> 00:25:11,840
uh I was just thinking about that like I

695
00:25:10,480 --> 00:25:14,159
never would have thought like the two of

696
00:25:11,840 --> 00:25:16,799
us technologists sitting around actually

697
00:25:14,159 --> 00:25:18,400
citing uh a book that you know Mackenzie

698
00:25:16,799 --> 00:25:20,240
consultants are known for. Feels so

699
00:25:18,400 --> 00:25:22,960
wrong. Yeah. I don't know aesthetically

700
00:25:20,240 --> 00:25:25,120
it feels terrible but yes

701
00:25:22,960 --> 00:25:27,679
seven powers. I guess we're entering

702
00:25:25,120 --> 00:25:29,279
this age of intelligence. I love uh that

703
00:25:27,679 --> 00:25:32,159
essay of yours. What do you think this

704
00:25:29,279 --> 00:25:33,919
era will mean for you know how we live

705
00:25:32,159 --> 00:25:37,679
uh how we work and how do we create

706
00:25:33,919 --> 00:25:39,520
value for each other as a society?

707
00:25:37,679 --> 00:25:42,559
You know, in some sense, the the whole

708
00:25:39,520 --> 00:25:46,240
arc of technology is one story, which is

709
00:25:42,559 --> 00:25:48,720
we discover more science, build better

710
00:25:46,240 --> 00:25:51,120
tools, all of society like builds the

711
00:25:48,720 --> 00:25:52,880
scaffolding a little bit higher. Um, and

712
00:25:51,120 --> 00:25:55,279
we we have this more impressive tool

713
00:25:52,880 --> 00:25:57,679
chain. And the whole point of it is that

714
00:25:55,279 --> 00:25:59,600
one person can do way more than they

715
00:25:57,679 --> 00:26:02,320
could before.

716
00:25:59,600 --> 00:26:05,039
And this has been going on for a long

717
00:26:02,320 --> 00:26:06,559
long time. each generation. Uh certainly

718
00:26:05,039 --> 00:26:08,720
I mean if you compare a person today

719
00:26:06,559 --> 00:26:10,880
from a person 100 or thousand years ago,

720
00:26:08,720 --> 00:26:13,760
one person is incredibly more capable

721
00:26:10,880 --> 00:26:16,400
and the kind of like social contract is

722
00:26:13,760 --> 00:26:19,679
that you put something, you know, you

723
00:26:16,400 --> 00:26:21,840
build the next layer of scaffolding. But

724
00:26:19,679 --> 00:26:23,679
what someone can do now with this new

725
00:26:21,840 --> 00:26:25,760
set of tools, with this, you know, this

726
00:26:23,679 --> 00:26:28,640
new layer that's been built in is pretty

727
00:26:25,760 --> 00:26:31,600
incredible. And

728
00:26:28,640 --> 00:26:33,279
I think the

729
00:26:31,600 --> 00:26:35,840
one of the things that will feel most

730
00:26:33,279 --> 00:26:37,760
different about

731
00:26:35,840 --> 00:26:40,640
these next 10 years versus these last 10

732
00:26:37,760 --> 00:26:42,640
years is how much a single person or a

733
00:26:40,640 --> 00:26:45,760
small group of people with a lot of

734
00:26:42,640 --> 00:26:47,679
agency can get done. And that is a

735
00:26:45,760 --> 00:26:50,080
bigger deal than it sounds like u

736
00:26:47,679 --> 00:26:51,919
because coordination costs are are huge.

737
00:26:50,080 --> 00:26:54,000
And when we can empower people with more

738
00:26:51,919 --> 00:26:55,919
knowledge, more tools, more resources,

739
00:26:54,000 --> 00:26:57,760
whatever,

740
00:26:55,919 --> 00:26:59,440
I think we won't just see like a little

741
00:26:57,760 --> 00:27:01,520
bit more stuff get built, but because of

742
00:26:59,440 --> 00:27:03,360
these kind of coordination costs across

743
00:27:01,520 --> 00:27:06,159
people, we'll see like a real stuff

744
00:27:03,360 --> 00:27:07,840
change. So I think the the amount that

745
00:27:06,159 --> 00:27:09,279
one person or small team get done, the

746
00:27:07,840 --> 00:27:11,039
satisfaction in doing that and and most

747
00:27:09,279 --> 00:27:12,880
importantly like the quality of stuff

748
00:27:11,039 --> 00:27:14,640
we'll all get for each other will be

749
00:27:12,880 --> 00:27:18,000
quite remarkable. When I think back

750
00:27:14,640 --> 00:27:20,159
about the OpenAI story, I often think

751
00:27:18,000 --> 00:27:22,640
about just the kind of

752
00:27:20,159 --> 00:27:24,480
key few tens of people that did the

753
00:27:22,640 --> 00:27:27,200
amazing work that led to what we all

754
00:27:24,480 --> 00:27:28,400
have now. But I try to remember that I

755
00:27:27,200 --> 00:27:29,840
always also have to think about like the

756
00:27:28,400 --> 00:27:32,000
tens of millions of people, maybe it's

757
00:27:29,840 --> 00:27:34,080
more uh throughout history that started

758
00:27:32,000 --> 00:27:35,840
like digging rocks out of the ground,

759
00:27:34,080 --> 00:27:37,440
figuring out how semiconductors work,

760
00:27:35,840 --> 00:27:39,520
building computers, building the

761
00:27:37,440 --> 00:27:41,200
internet, and on and on and on that let

762
00:27:39,520 --> 00:27:42,559
this small group be able to work at such

763
00:27:41,200 --> 00:27:44,480
a high level of impact that they never

764
00:27:42,559 --> 00:27:46,799
would have been able to do without the

765
00:27:44,480 --> 00:27:49,120
collective output of society.

766
00:27:46,799 --> 00:27:51,200
Is it surprising to you to what degree?

767
00:27:49,120 --> 00:27:53,840
I mean, this room is you're preaching to

768
00:27:51,200 --> 00:27:55,440
the room of the converted, but uh this

769
00:27:53,840 --> 00:27:57,039
is awesome, by the way. I mean, this is

770
00:27:55,440 --> 00:27:59,840
like the collected set of people who are

771
00:27:57,039 --> 00:28:02,159
going to go create the future, but um

772
00:27:59,840 --> 00:28:03,520
there's, you know, yeah, there's there's

773
00:28:02,159 --> 00:28:05,360
maybe like never been a gathering like

774
00:28:03,520 --> 00:28:07,120
this in one place before. This is this

775
00:28:05,360 --> 00:28:10,080
is very cool to see. But at the same

776
00:28:07,120 --> 00:28:13,520
time, you know, we're in some ways, uh,

777
00:28:10,080 --> 00:28:15,840
this is the leading cutting edge of all

778
00:28:13,520 --> 00:28:18,240
of society because there are seven and a

779
00:28:15,840 --> 00:28:20,000
half billion people who probably h, you

780
00:28:18,240 --> 00:28:22,320
know, have not even tried this stuff

781
00:28:20,000 --> 00:28:25,039
yet. And not only that, their main

782
00:28:22,320 --> 00:28:27,440
interaction with it is uh, that it

783
00:28:25,039 --> 00:28:28,640
doesn't work, that it hallucinates.

784
00:28:27,440 --> 00:28:31,200
You know, what what do you have to say

785
00:28:28,640 --> 00:28:33,520
to the 3,000 people in front of you

786
00:28:31,200 --> 00:28:36,000
right now? Just this is the the thin

787
00:28:33,520 --> 00:28:38,000
edge of the spear. We are literally

788
00:28:36,000 --> 00:28:40,640
teaching people and giving people this

789
00:28:38,000 --> 00:28:42,559
technology. First of all, that's like a

790
00:28:40,640 --> 00:28:43,919
great place to be in. Um, one of the one

791
00:28:42,559 --> 00:28:45,919
of the most fun things about working at

792
00:28:43,919 --> 00:28:48,000
YC is you get to like live on the

793
00:28:45,919 --> 00:28:50,720
leading edge and you get to be around

794
00:28:48,000 --> 00:28:52,320
the people who are the advanced guard.

795
00:28:50,720 --> 00:28:53,919
Um, and that's just like a fun way to

796
00:28:52,320 --> 00:28:55,279
live your life and you get to see what's

797
00:28:53,919 --> 00:28:57,360
coming and, you know, hopefully have

798
00:28:55,279 --> 00:28:59,520
some small amount of input into shaping

799
00:28:57,360 --> 00:29:01,679
it. Uh,

800
00:28:59,520 --> 00:29:04,080
but I don't know. I think AI is like

801
00:29:01,679 --> 00:29:06,159
somewhat mainstream right now. I the the

802
00:29:04,080 --> 00:29:07,840
the negative the way that it's not is

803
00:29:06,159 --> 00:29:10,880
most people still think of AI as chat

804
00:29:07,840 --> 00:29:13,440
GPT and a lot of people use chat GPT but

805
00:29:10,880 --> 00:29:14,799
they use it like a a chatbot and they

806
00:29:13,440 --> 00:29:16,559
have not yet wrapped their head around

807
00:29:14,799 --> 00:29:19,520
what's what's coming next and probably

808
00:29:16,559 --> 00:29:21,200
you all have but

809
00:29:19,520 --> 00:29:22,720
I don't know it's like a great privilege

810
00:29:21,200 --> 00:29:25,840
to get to live a little bit in the

811
00:29:22,720 --> 00:29:28,399
future and uh you know go build stuff

812
00:29:25,840 --> 00:29:30,720
for everybody else coming along. So,

813
00:29:28,399 --> 00:29:32,960
you're sort of one of the best people in

814
00:29:30,720 --> 00:29:35,440
the world at um bringing together the

815
00:29:32,960 --> 00:29:37,279
smartest people. Um what are some of the

816
00:29:35,440 --> 00:29:39,440
hardest lessons you've had to learn

817
00:29:37,279 --> 00:29:42,000
about hiring? A lot of the people in

818
00:29:39,440 --> 00:29:44,960
this room like they have never managed a

819
00:29:42,000 --> 00:29:47,200
person before, let alone gotten someone

820
00:29:44,960 --> 00:29:50,240
to quit their, you know, six to seven

821
00:29:47,200 --> 00:29:53,600
figure job at some big company to come

822
00:29:50,240 --> 00:29:57,679
work on their revolution. hiring really

823
00:29:53,600 --> 00:30:00,880
smart people who are clearly really

824
00:29:57,679 --> 00:30:03,840
driven and really productive and can

825
00:30:00,880 --> 00:30:06,240
work as part of a team I think does get

826
00:30:03,840 --> 00:30:09,679
you

827
00:30:06,240 --> 00:30:12,640
90% of the way there and the degree to

828
00:30:09,679 --> 00:30:17,279
which people focus on other things to

829
00:30:12,640 --> 00:30:18,799
hire for always surprises me. So I think

830
00:30:17,279 --> 00:30:23,039
you know given that we can't do the full

831
00:30:18,799 --> 00:30:25,840
45 minutes right now really smart people

832
00:30:23,039 --> 00:30:27,520
driven curious self-motivated

833
00:30:25,840 --> 00:30:29,279
hardworking

834
00:30:27,520 --> 00:30:31,440
uh like good track record of

835
00:30:29,279 --> 00:30:33,520
accomplishment and can work really well

836
00:30:31,440 --> 00:30:35,760
as part of a team and sort of aligned

837
00:30:33,520 --> 00:30:38,000
with the company's vision and so

838
00:30:35,760 --> 00:30:39,760
everybody's at least going for the same

839
00:30:38,000 --> 00:30:42,880
the same direction that works pretty

840
00:30:39,760 --> 00:30:45,200
well. I mean by uh strong track record

841
00:30:42,880 --> 00:30:46,960
do you mean the person who's like you

842
00:30:45,200 --> 00:30:49,919
know sort of been an administrator and

843
00:30:46,960 --> 00:30:52,960
had like the the you know top name at

844
00:30:49,919 --> 00:30:55,279
the top institution for 20 years or do

845
00:30:52,960 --> 00:30:57,520
you mean like because you you went the

846
00:30:55,279 --> 00:31:00,799
other way? I

847
00:30:57,520 --> 00:31:02,320
I don't especially early in a startup I

848
00:31:00,799 --> 00:31:04,720
don't believe in hiring those people.

849
00:31:02,320 --> 00:31:07,600
There experience is valuable and there

850
00:31:04,720 --> 00:31:10,720
are times where you you really need that

851
00:31:07,600 --> 00:31:12,559
but I have not had success and to be

852
00:31:10,720 --> 00:31:15,679
frank like YC has not had that much

853
00:31:12,559 --> 00:31:18,960
success trying to start with like the

854
00:31:15,679 --> 00:31:20,480
very senior eminent administrator as one

855
00:31:18,960 --> 00:31:22,559
of the like you know as the first hire

856
00:31:20,480 --> 00:31:25,679
in a startup. Um I would I would take

857
00:31:22,559 --> 00:31:29,279
like young scrappy

858
00:31:25,679 --> 00:31:31,760
but clearly like get stuff done um over

859
00:31:29,279 --> 00:31:34,000
the person who has like the extremely

860
00:31:31,760 --> 00:31:35,279
polished track record. There will come a

861
00:31:34,000 --> 00:31:37,600
time where you need some of those people

862
00:31:35,279 --> 00:31:39,200
later. But I don't know how you do it,

863
00:31:37,600 --> 00:31:40,640
but when I when I was like reading YC

864
00:31:39,200 --> 00:31:42,559
applications, I would like never look at

865
00:31:40,640 --> 00:31:44,000
the resume items. You know, you worked

866
00:31:42,559 --> 00:31:45,360
at like Google or went to this college.

867
00:31:44,000 --> 00:31:46,640
I never cared. I would always go right

868
00:31:45,360 --> 00:31:49,519
to like what's the most impressive stuff

869
00:31:46,640 --> 00:31:51,600
you've done? And then sometimes I would

870
00:31:49,519 --> 00:31:52,799
like not be convinced by that and go

871
00:31:51,600 --> 00:31:54,720
look at the resume. But that was always

872
00:31:52,799 --> 00:31:57,279
like a backup to me as a secondary

873
00:31:54,720 --> 00:31:58,960
thing. So sort of look look at what

874
00:31:57,279 --> 00:32:01,600
they've actually what they've coded,

875
00:31:58,960 --> 00:32:04,240
what they've built, like their velocity,

876
00:32:01,600 --> 00:32:06,320
how they think about problems and solve

877
00:32:04,240 --> 00:32:07,600
them. I see PB back there. He has this

878
00:32:06,320 --> 00:32:08,960
quote, I hope it's his quote because

879
00:32:07,600 --> 00:32:12,080
I've attributed to him a bunch of times

880
00:32:08,960 --> 00:32:13,600
of hire for slope, not y intercept. And

881
00:32:12,080 --> 00:32:16,240
I think that's just like unbelievably

882
00:32:13,600 --> 00:32:18,399
great advice. Let's talk about um being

883
00:32:16,240 --> 00:32:20,720
CEO of OpenAI. What are some of the

884
00:32:18,399 --> 00:32:23,519
hardest lessons there just overall? I

885
00:32:20,720 --> 00:32:25,760
don't recommend it. Um,

886
00:32:23,519 --> 00:32:28,000
no one single challenge would be that

887
00:32:25,760 --> 00:32:30,159
hard, but the number of things we have

888
00:32:28,000 --> 00:32:33,200
to do at the same time and the kind of

889
00:32:30,159 --> 00:32:35,279
like number of other big companies that

890
00:32:33,200 --> 00:32:36,960
are gunning for us in various ways, it's

891
00:32:35,279 --> 00:32:39,120
just like more context than I thought it

892
00:32:36,960 --> 00:32:41,679
was possible to handle at once and more

893
00:32:39,120 --> 00:32:43,519
sort of like switching from like big big

894
00:32:41,679 --> 00:32:46,240
decision to like totally unrelated but

895
00:32:43,519 --> 00:32:48,640
also huge decision. Looking ahead 10 to

896
00:32:46,240 --> 00:32:50,399
20 years, what are you sort of most

897
00:32:48,640 --> 00:32:53,919
personally

898
00:32:50,399 --> 00:32:55,919
excited about? You know, and what should

899
00:32:53,919 --> 00:32:58,080
uh people be building now to make that

900
00:32:55,919 --> 00:32:59,519
future possible? You know, there are

901
00:32:58,080 --> 00:33:01,600
people who are scientists, there are

902
00:32:59,519 --> 00:33:04,399
people who are software engineers, there

903
00:33:01,600 --> 00:33:06,720
are people who are I mean this is an all

904
00:33:04,399 --> 00:33:10,159
technical crowd.

905
00:33:06,720 --> 00:33:12,080
Look, there there's a lot um you know,

906
00:33:10,159 --> 00:33:13,360
in 10 or 20 years, unless something goes

907
00:33:12,080 --> 00:33:16,000
hugely wrong, we'll have like

908
00:33:13,360 --> 00:33:17,519
unimaginable super intelligence. And I'm

909
00:33:16,000 --> 00:33:19,360
very excited to see how that goes.

910
00:33:17,519 --> 00:33:22,880
Forced to pick one thing to just not

911
00:33:19,360 --> 00:33:24,960
leave it as like a vague answer. Um I

912
00:33:22,880 --> 00:33:27,200
think AI for science is what I'm

913
00:33:24,960 --> 00:33:29,039
personally most excited about. I I am a

914
00:33:27,200 --> 00:33:32,240
believer that to a first order

915
00:33:29,039 --> 00:33:33,840
approximation all long-term sustainable

916
00:33:32,240 --> 00:33:35,120
economic growth in the world like

917
00:33:33,840 --> 00:33:36,960
everything that leads to people's lives

918
00:33:35,120 --> 00:33:38,799
getting better um is basically

919
00:33:36,960 --> 00:33:40,080
discovering new science and having

920
00:33:38,799 --> 00:33:41,840
reasonably good governance and

921
00:33:40,080 --> 00:33:44,399
institutions so that that science can

922
00:33:41,840 --> 00:33:47,440
get developed and shared with the world.

923
00:33:44,399 --> 00:33:50,640
But if we could vastly increase the rate

924
00:33:47,440 --> 00:33:52,559
of new scientific discovery with AI, uh

925
00:33:50,640 --> 00:33:55,200
I believe that would compound to just

926
00:33:52,559 --> 00:33:57,120
incredible increases and wonders for

927
00:33:55,200 --> 00:33:59,440
everyone's lives. So I think I'd pick

928
00:33:57,120 --> 00:34:00,799
that on that time frame. I guess uh one

929
00:33:59,440 --> 00:34:04,640
of the things I've been always really

930
00:34:00,799 --> 00:34:07,039
impressed by is uh you know you were you

931
00:34:04,640 --> 00:34:09,679
you know personally recruited Helon to

932
00:34:07,039 --> 00:34:11,200
come do Y Combinator and uh they're

933
00:34:09,679 --> 00:34:14,960
doing incredible things over on the

934
00:34:11,200 --> 00:34:16,560
Fusion side. Um, was that something that

935
00:34:14,960 --> 00:34:19,359
you were thinking about even all the way

936
00:34:16,560 --> 00:34:21,839
back then or you know obviously energy

937
00:34:19,359 --> 00:34:23,280
and climate was sort of a part of uh,

938
00:34:21,839 --> 00:34:25,520
you know, what everyone's worried about

939
00:34:23,280 --> 00:34:26,960
even back then. But this is a little bit

940
00:34:25,520 --> 00:34:30,000
embarrassing. I've been obsessed with

941
00:34:26,960 --> 00:34:31,280
energy and AI as like the two the things

942
00:34:30,000 --> 00:34:32,720
that I thought would be the two most

943
00:34:31,280 --> 00:34:35,520
important things or at least the ones I

944
00:34:32,720 --> 00:34:37,440
was going to be most that I felt most

945
00:34:35,520 --> 00:34:39,440
passionate about for a long time. and

946
00:34:37,440 --> 00:34:41,200
and really like the two areas that I I

947
00:34:39,440 --> 00:34:44,639
knew I wanted to like concentrate time

948
00:34:41,200 --> 00:34:47,119
and capital towards. I I cannot recall

949
00:34:44,639 --> 00:34:48,720
ever thinking until like after starting

950
00:34:47,119 --> 00:34:50,800
Open AI that they were going to be so

951
00:34:48,720 --> 00:34:53,520
obviously related

952
00:34:50,800 --> 00:34:55,119
that you know that that energy would be

953
00:34:53,520 --> 00:34:57,599
the eventually the fundamental limiter

954
00:34:55,119 --> 00:34:59,359
on how much intelligence we could have.

955
00:34:57,599 --> 00:35:01,119
And I don't know how I missed that

956
00:34:59,359 --> 00:35:02,880
because I usually am good at thinking

957
00:35:01,119 --> 00:35:04,160
about things like that but I I really

958
00:35:02,880 --> 00:35:05,280
did think of them as like very

959
00:35:04,160 --> 00:35:06,960
independent. you know, we were going to

960
00:35:05,280 --> 00:35:08,960
need AI to have all the ideas, energy to

961
00:35:06,960 --> 00:35:11,280
make all the stuff happen in the world.

962
00:35:08,960 --> 00:35:12,640
And I obviously right after starting

963
00:35:11,280 --> 00:35:15,599
open AI, I got obsessed with meaning

964
00:35:12,640 --> 00:35:17,760
energy for AI. But like pre205, I think

965
00:35:15,599 --> 00:35:19,359
I thought of them as orthogonal vectors.

966
00:35:17,760 --> 00:35:21,359
I mean, you I'm sure you've seen that

967
00:35:19,359 --> 00:35:23,440
chart that um you know, all the

968
00:35:21,359 --> 00:35:27,280
effective accelerationists in the room

969
00:35:23,440 --> 00:35:29,119
have seen around basically having a high

970
00:35:27,280 --> 00:35:30,560
standard of living like the sort of

971
00:35:29,119 --> 00:35:31,680
really it's I'm obsessed with this

972
00:35:30,560 --> 00:35:34,480
chart. I've been obsessed with that

973
00:35:31,680 --> 00:35:36,640
chart a long time. um it's directly uh

974
00:35:34,480 --> 00:35:39,200
related to the amount of energy that any

975
00:35:36,640 --> 00:35:40,800
given person has access to. Yeah. I I

976
00:35:39,200 --> 00:35:43,920
think this is one of the most amazing

977
00:35:40,800 --> 00:35:45,920
charts over a long like long long period

978
00:35:43,920 --> 00:35:47,440
of human history is the correlation of

979
00:35:45,920 --> 00:35:50,640
quality of life and abundance of energy

980
00:35:47,440 --> 00:35:52,240
and cost of energy. So that was that

981
00:35:50,640 --> 00:35:54,000
chart and charts like that were a

982
00:35:52,240 --> 00:35:56,160
significant reason that I got obsessed

983
00:35:54,000 --> 00:35:58,560
with energy in the first place. It it is

984
00:35:56,160 --> 00:36:01,520
it is just this like crazy high impact

985
00:35:58,560 --> 00:36:03,359
thing. It sounds like it wasn't entirely

986
00:36:01,520 --> 00:36:05,680
uh interdependent. It was more you had

987
00:36:03,359 --> 00:36:07,359
twin interest, but you've literally

988
00:36:05,680 --> 00:36:09,520
woven them together. I had like the one

989
00:36:07,359 --> 00:36:11,440
interest of like radical abundance and

990
00:36:09,520 --> 00:36:13,599
just like what what what were the kind

991
00:36:11,440 --> 00:36:15,280
of technological leverage points to just

992
00:36:13,599 --> 00:36:17,040
like make the future like wildly

993
00:36:15,280 --> 00:36:18,720
different and better and these is the

994
00:36:17,040 --> 00:36:21,280
two kind of key things for that. But not

995
00:36:18,720 --> 00:36:22,720
as not as much as the same vector. Now I

996
00:36:21,280 --> 00:36:24,320
think a lot about like how much energy

997
00:36:22,720 --> 00:36:25,839
can we actually build on Earth before we

998
00:36:24,320 --> 00:36:27,839
just heat the planet too much from

999
00:36:25,839 --> 00:36:29,680
running the GPUs and like how long can

1000
00:36:27,839 --> 00:36:33,040
we go before we have to put all the GPUs

1001
00:36:29,680 --> 00:36:34,240
in space. Um but at the time yeah I

1002
00:36:33,040 --> 00:36:35,920
really thought of them differently. I

1003
00:36:34,240 --> 00:36:39,040
mean it seems like one of the uh

1004
00:36:35,920 --> 00:36:41,920
defining beliefs that technologists uh

1005
00:36:39,040 --> 00:36:44,000
uniquely ideally have that they believe

1006
00:36:41,920 --> 00:36:45,839
that we can actually create that sort of

1007
00:36:44,000 --> 00:36:48,240
abundance. You know, if you have

1008
00:36:45,839 --> 00:36:51,520
intelligence on tap and then you have

1009
00:36:48,240 --> 00:36:54,160
energy on tap, then um how does that go?

1010
00:36:51,520 --> 00:36:57,520
It's like, you know, all uh all watched

1011
00:36:54,160 --> 00:36:59,599
over by machines of loving grace. I I've

1012
00:36:57,520 --> 00:37:01,760
never been to one of those deg

1013
00:36:59,599 --> 00:37:03,200
conferences in Europe or whatever. Um

1014
00:37:01,760 --> 00:37:04,960
but I've always kind of wanted to go to

1015
00:37:03,200 --> 00:37:06,880
one. This is the anti-dgrowth. This is

1016
00:37:04,960 --> 00:37:09,280
the anti-drowth conference. Totally. But

1017
00:37:06,880 --> 00:37:12,079
I would like love to be like sitting,

1018
00:37:09,280 --> 00:37:14,079
you know, in the dark in the cold with

1019
00:37:12,079 --> 00:37:15,599
no one pulling out their phones and just

1020
00:37:14,079 --> 00:37:17,040
like talking about how horrible

1021
00:37:15,599 --> 00:37:19,040
everything was and there was no hope.

1022
00:37:17,040 --> 00:37:21,359
Like I would love to experience that

1023
00:37:19,040 --> 00:37:23,680
mindset once because I've never felt it.

1024
00:37:21,359 --> 00:37:25,359
Um, and I think it is like it it is one

1025
00:37:23,680 --> 00:37:27,119
of the movements that has been ever

1026
00:37:25,359 --> 00:37:29,520
hardest for me to identify with.

1027
00:37:27,119 --> 00:37:32,079
Obviously, this is like my crew and my

1028
00:37:29,520 --> 00:37:35,440
world, but the the sort of like the

1029
00:37:32,079 --> 00:37:37,119
optimism of startups of San Francisco,

1030
00:37:35,440 --> 00:37:40,400
of the technology industry, of AI, of

1031
00:37:37,119 --> 00:37:44,240
what all y'all y'all will do, uh like

1032
00:37:40,400 --> 00:37:46,160
that is that is like the natural space

1033
00:37:44,240 --> 00:37:48,240
my brain abides and it's very hard for

1034
00:37:46,160 --> 00:37:49,839
me to really empathize with the other

1035
00:37:48,240 --> 00:37:51,599
side of that, but I'm pretty sure we're

1036
00:37:49,839 --> 00:37:54,480
right and they're wrong. How do we get

1037
00:37:51,599 --> 00:37:57,200
there though, right? you this incredible

1038
00:37:54,480 --> 00:38:00,320
vision of technology actually creating

1039
00:37:57,200 --> 00:38:02,240
for abundance for others. I mean you've

1040
00:38:00,320 --> 00:38:04,480
already done so much but you know point

1041
00:38:02,240 --> 00:38:06,160
us the way like you know how how else do

1042
00:38:04,480 --> 00:38:08,079
we get there? How do we make it faster?

1043
00:38:06,160 --> 00:38:11,280
You know does government play a role in

1044
00:38:08,079 --> 00:38:14,880
this? Almost just about five years ago

1045
00:38:11,280 --> 00:38:17,680
like pretty much this week we put GPT3

1046
00:38:14,880 --> 00:38:21,040
into an API and people started playing

1047
00:38:17,680 --> 00:38:23,920
with it and it was barely usable. It was

1048
00:38:21,040 --> 00:38:25,520
quite embarrassing. Um and in five years

1049
00:38:23,920 --> 00:38:27,680
we have gone from this like thing that

1050
00:38:25,520 --> 00:38:29,599
could barely write a sentence to a thing

1051
00:38:27,680 --> 00:38:34,240
that is like you know PhD level

1052
00:38:29,599 --> 00:38:35,520
intelligence in most areas. Um five more

1053
00:38:34,240 --> 00:38:37,280
years I think we'll be able to maintain

1054
00:38:35,520 --> 00:38:40,800
the same rate of progress. And I think

1055
00:38:37,280 --> 00:38:44,160
if we do that if we also build out the

1056
00:38:40,800 --> 00:38:46,240
infrastructure to serve that to people

1057
00:38:44,160 --> 00:38:48,160
then everybody in this room will figure

1058
00:38:46,240 --> 00:38:50,480
out how to take that technology and

1059
00:38:48,160 --> 00:38:53,200
adapt it to what everybody needs. The

1060
00:38:50,480 --> 00:38:55,119
the analogy I like most for AI is the

1061
00:38:53,200 --> 00:38:57,440
transistor like the historical tech

1062
00:38:55,119 --> 00:38:59,920
technical analogy. You know, some people

1063
00:38:57,440 --> 00:39:02,160
figured out like a new really important

1064
00:38:59,920 --> 00:39:04,320
scientific discovery

1065
00:39:02,160 --> 00:39:06,160
and society, the economy, whatever you

1066
00:39:04,320 --> 00:39:07,920
want to call it, just got to work, just

1067
00:39:06,160 --> 00:39:09,680
did its thing. The magic of that just

1068
00:39:07,920 --> 00:39:12,400
figured out how to make incredible value

1069
00:39:09,680 --> 00:39:14,800
for people and really over a fairly

1070
00:39:12,400 --> 00:39:17,440
short period of decades significantly

1071
00:39:14,800 --> 00:39:18,720
ramp up quality of life. I think this

1072
00:39:17,440 --> 00:39:19,920
will be even faster and steeper than

1073
00:39:18,720 --> 00:39:21,680
that. But I think it'll go in

1074
00:39:19,920 --> 00:39:23,440
directionally the same way. You know, we

1075
00:39:21,680 --> 00:39:24,640
need to make the great technology,

1076
00:39:23,440 --> 00:39:26,000
figure out the remaining scientific

1077
00:39:24,640 --> 00:39:28,000
stuff, which I don't think there's much

1078
00:39:26,000 --> 00:39:29,920
left. We need to figure out how to build

1079
00:39:28,000 --> 00:39:32,160
out the infrastructure

1080
00:39:29,920 --> 00:39:34,000
that you all will need to be able to

1081
00:39:32,160 --> 00:39:35,760
service and then you all have got to go

1082
00:39:34,000 --> 00:39:38,720
figure out what what people in the world

1083
00:39:35,760 --> 00:39:41,920
need um with this new magic. So, let's

1084
00:39:38,720 --> 00:39:45,280
uh flash back to 2005. Uh the very first

1085
00:39:41,920 --> 00:39:47,520
batch of Y Combinator. Um, how did you

1086
00:39:45,280 --> 00:39:49,040
hear about Paul Graham? You were reading

1087
00:39:47,520 --> 00:39:50,480
his essays. I was reading his essays.

1088
00:39:49,040 --> 00:39:52,240
So, I'd heard about like he kind of had

1089
00:39:50,480 --> 00:39:54,960
this cult following on the internet, but

1090
00:39:52,240 --> 00:39:56,560
I heard about uh what was then called

1091
00:39:54,960 --> 00:39:59,520
the summer founders program and now it's

1092
00:39:56,560 --> 00:40:02,400
just called Y Cominator from Blake Ross

1093
00:39:59,520 --> 00:40:05,200
who I lived in the same freshman dorm

1094
00:40:02,400 --> 00:40:07,520
with and posted about it on Facebook.

1095
00:40:05,200 --> 00:40:10,160
And then um I think Paul said, "Oh,

1096
00:40:07,520 --> 00:40:12,400
you're a freshman. You know that there's

1097
00:40:10,160 --> 00:40:14,720
like another batch coming." And what did

1098
00:40:12,400 --> 00:40:16,079
you reply to him by email with? You

1099
00:40:14,720 --> 00:40:18,079
know, funny you bring that up. I just

1100
00:40:16,079 --> 00:40:20,720
dug up the email like a couple of days

1101
00:40:18,079 --> 00:40:24,720
ago because I I felt I had been misqued

1102
00:40:20,720 --> 00:40:26,800
over time. I'm curious. And

1103
00:40:24,720 --> 00:40:28,000
the the his telling of the story is I

1104
00:40:26,800 --> 00:40:30,640
said like I'm a sophomore and I'm

1105
00:40:28,000 --> 00:40:32,720
coming. But I wrote a much nicer thing

1106
00:40:30,640 --> 00:40:34,079
than that. It was like oh maybe there

1107
00:40:32,720 --> 00:40:35,599
was some misunderstanding. You know,

1108
00:40:34,079 --> 00:40:36,800
actually I'm a sophomore and I can still

1109
00:40:35,599 --> 00:40:38,880
make it and I would like love to if

1110
00:40:36,800 --> 00:40:41,599
that's still okay to come the next day.

1111
00:40:38,880 --> 00:40:43,599
So I in some ways you know the wild

1112
00:40:41,599 --> 00:40:47,280
thing is you're sitting in front of uh

1113
00:40:43,599 --> 00:40:50,079
3,000 people who kind of was you know

1114
00:40:47,280 --> 00:40:54,960
they they are sitting where you were

1115
00:40:50,079 --> 00:40:57,599
back in 2005. Um what would you say to

1116
00:40:54,960 --> 00:40:59,200
you know the Sam Alman from that time

1117
00:40:57,599 --> 00:41:00,720
you know given what you know all the

1118
00:40:59,200 --> 00:41:02,720
things you've seen all the things you've

1119
00:41:00,720 --> 00:41:04,560
learned since like what are the things

1120
00:41:02,720 --> 00:41:06,960
that you're most surprised you didn't

1121
00:41:04,560 --> 00:41:08,480
know that I mean it just took I mean

1122
00:41:06,960 --> 00:41:10,720
you've been through it you know like

1123
00:41:08,480 --> 00:41:12,960
you've you've done it. I wish someone

1124
00:41:10,720 --> 00:41:16,079
had like taught me the importance of

1125
00:41:12,960 --> 00:41:18,960
like conviction and

1126
00:41:16,079 --> 00:41:20,640
resilience over a long period of time.

1127
00:41:18,960 --> 00:41:22,160
People don't really talk about how hard

1128
00:41:20,640 --> 00:41:23,599
that is. it's like easy for a little

1129
00:41:22,160 --> 00:41:26,400
while, but your reserves kind of like

1130
00:41:23,599 --> 00:41:28,720
wear down on it and how to how to keep

1131
00:41:26,400 --> 00:41:30,720
that going for a long period of time.

1132
00:41:28,720 --> 00:41:32,880
Um,

1133
00:41:30,720 --> 00:41:34,720
also just sort of like trust that it's

1134
00:41:32,880 --> 00:41:36,079
eventually going to work out. Like

1135
00:41:34,720 --> 00:41:39,520
obviously my first startup didn't work

1136
00:41:36,079 --> 00:41:41,920
that well. Um, I think a lot of people

1137
00:41:39,520 --> 00:41:43,839
kind of give up after one failed

1138
00:41:41,920 --> 00:41:46,880
startup, but startups don't work out all

1139
00:41:43,839 --> 00:41:48,560
the time. Uh, and learning how to keep

1140
00:41:46,880 --> 00:41:52,240
going through that, keep working through

1141
00:41:48,560 --> 00:41:54,720
that is is I think really important.

1142
00:41:52,240 --> 00:41:57,359
Developing like trust in your own

1143
00:41:54,720 --> 00:41:59,119
instincts and increasing that trust as

1144
00:41:57,359 --> 00:42:00,480
you refine your decision-making and

1145
00:41:59,119 --> 00:42:02,880
instincts over time. I think that's

1146
00:42:00,480 --> 00:42:04,640
really important. Kind of courage to

1147
00:42:02,880 --> 00:42:05,920
work on stuff that is out of fashion but

1148
00:42:04,640 --> 00:42:07,760
is what you believe and what you care

1149
00:42:05,920 --> 00:42:09,599
about. Uh, I think that's really

1150
00:42:07,760 --> 00:42:10,800
important. I had a kid recently and the

1151
00:42:09,599 --> 00:42:12,640
thing everyone tells you when you have a

1152
00:42:10,800 --> 00:42:14,400
kid is that it is the best thing you

1153
00:42:12,640 --> 00:42:16,240
will ever do but also it is the hardest

1154
00:42:14,400 --> 00:42:17,680
thing you will ever do. Like the the

1155
00:42:16,240 --> 00:42:19,839
good parts are much better than you can

1156
00:42:17,680 --> 00:42:22,640
imagine. Um the hard parts are much

1157
00:42:19,839 --> 00:42:24,720
harder. That is all totally true. And

1158
00:42:22,640 --> 00:42:26,720
that is also basically what I feel like

1159
00:42:24,720 --> 00:42:28,640
being an entrepreneur is like the good

1160
00:42:26,720 --> 00:42:31,119
parts are really great better than you

1161
00:42:28,640 --> 00:42:33,040
think and the hard parts are like

1162
00:42:31,119 --> 00:42:35,280
shockingly much harder than anyone can

1163
00:42:33,040 --> 00:42:36,400
can express in a way that uh makes any

1164
00:42:35,280 --> 00:42:41,440
sense to you and you have to just keep

1165
00:42:36,400 --> 00:42:44,599
going. Sam Alman. Everyone, thank you.

1166
00:42:41,440 --> 00:42:44,599
Thank you.

